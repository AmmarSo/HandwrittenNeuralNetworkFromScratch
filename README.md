ğŸ§  Neural Network from Scratch - MNIST Digit Recognition

This project implements a neural network from scratch (without using machine learning libraries like TensorFlow or PyTorch) to recognize handwritten digits from the MNIST dataset.

ğŸ“˜ Read the full article on Medium

ğŸ“‚ Project Structure

â”œâ”€â”€ src/                # Source code for the neural network
â”‚   â”œâ”€â”€ neuron.py       # Neuron class
â”‚   â”œâ”€â”€ layer.py        # Layer class
â”‚   â”œâ”€â”€ network.py      # Network architecture and training
â”‚   â”œâ”€â”€ train.py        # Training loop
â”‚   â””â”€â”€ test.py         # Evaluation code
â”œâ”€â”€ data/               # (Optional) Data loading or storage
â”œâ”€â”€ notebooks/          # Jupyter notebooks (for experiments or visualizations)
â”œâ”€â”€ models/             # Saved model weights (optional)
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ .gitignore          # Files ignored by Git

ğŸš€ Getting Started

1. Clone the repository

git clone https://github.com/<your-username>/mnist-neural-network-from-scratch.git
cd mnist-neural-network-from-scratch

2. Create a virtual environment (optional)

python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

3. Install dependencies

pip install -r requirements.txt

4. Run the training

python src/train.py

5. Evaluate the model

python src/test.py

ğŸ“Š Example Results

Metric

Value

Accuracy

~92%

Loss

Decreasing

Hidden Layers

2

Activation

Sigmoid

ğŸ§ª Technologies Used

Python 3.x

NumPy

MNIST Dataset (from Yann LeCun's website)

ğŸ§  Core Concepts

Forward Propagation

Backpropagation

Gradient Descent

Weight & Bias Initialization

Cost Function (Mean Squared Error)

ğŸ“„ License

MIT License Â© 2025 Ammar Souchon

âœï¸ Author

Ammar SouchonFollow me on Medium

ğŸŒŸ Show your support

If you like this project, please consider leaving a â­ on the repository!

